import pandas as pd
import os
import time
from datetime import date
import re
from gender_guesser_br import Genero


PASTA_ORIGEM_ONEDRIVE = r"C:\Users\LuisGuilhermeMoraesd\Nefroclinicas Servi√ßo de Nefrologia e Dialise Ltda\Nefroclinicas - 07 - DADOS (1)"
PASTA_TRABALHO = r"C:\Users\LuisGuilhermeMoraesd\OneDrive - Nefroclinicas Servi√ßo de Nefrologia e Dialise Ltda\√Årea de Trabalho\Limpar_Dados"

# Nomes dos arquivos de sa√≠da e auxiliares
ARQUIVO_SEXO = os.path.join(PASTA_TRABALHO, "Nomes_e_Sexo_Inferido.xlsx")
ARQUIVO_INTERMEDIARIO_CSV = os.path.join(PASTA_TRABALHO, "Base_BI_Consolidada2.csv")
ARQUIVO_FINAL_MODELADO = os.path.join(PASTA_TRABALHO, "Base_MODELADA_PowerBI_V4.xlsx")
# Arquivos Auxiliares (Faltas e Absente√≠smo)
ARQUIVO_FALTAS = os.path.join(PASTA_TRABALHO, "faltas.csv")
ARQUIVO_ABS = os.path.join(PASTA_TRABALHO, "ABS.csv")


# Lista EXATA das colunas a serem extra√≠das e usadas no modelo
COLUNAS_DESEJADAS = [
'Nome', 'Sexo','CPF', 'Empresa', 'Cadastro', 'Admiss√£o', 'Cargo', 'C.Custo', 
'Descri√ß√£o (C.Custo)', 'Data Afastamento', 'T√≠tulo Reduzido (Cargo)', 
'Descri√ß√£o (Ra√ßa/Etnia)', 'Descri√ß√£o (Cat. eSocial)', 'Causa', 
'Descri√ß√£o (Causa)', 'Escala', 'Descri√ß√£o (Escala)', 'Filial', 
'Apelido (Filial)', 'C√≥digo Fornecedor', 'Descri√ß√£o (Motivo Alt. Sal√°rio)',
'Data Adicionais', 'Data Aposentadoria', 'Data Cargo', 'Data C.Custo', 
'Data Ult. Alt. Cat.', 'Data de Chegada', 'Data Escala', 'Data Estabilidade', 
'Data Escala VTR', 'Data Filial', 'Data Hist√≥rico de Contrato', 'Data Inclus√£o', 
'Data Local', 'Nascimento', 'Op√ß√£o FGTS', 'Data Posto', 'Data Ass. PPR', 
'Data de Reintegra√ß√£o', 'Data Sal√°rio', 'Data Cat. SEFIP', '√öltima Simula√ß√£o', 
'Data Sindicato', 'Data FGTS', 'Data V√≠nculo', 'Cadastramento PIS', 
'Dependentes IR', 'Dependentes Saf', 'Dep. Saldo FGTS', 'Estado Civil', 
'Descri√ß√£o (Estado Civil)', 'Instru√ß√£o', 'Descri√ß√£o (Instru√ß√£o)', 
'Nome (Empresa)', 'Nome (Cadastro O. Contrato)', 'Nome (Empresa O. Contrato)',
'Descri√ß√£o (Tipo O. Contrato)', '% Desempenho', '% Insalubridade', 
'% Base IR Transportista', '% ISS', '% FGTS', 'Per√≠odo Pagto', 
'Descri√ß√£o (Per√≠odo Pagto)', '% Periculosidade', '% Reajuste', 
'% Base INSS Transportista', 'Ra√ßa/Etnia', 'Recebe 13¬∞ Sal√°rio', 'Situa√ß√£o', 
'Descri√ß√£o (Situa√ß√£o)', 'Descri√ß√£o (T. Adm)', 'Descri√ß√£o (T. Contrato)'
]


# =======================================================================
# 2. FUN√á√ïES DE SUPORTE
# =======================================================================

# -----------------------------------------------------------------------
# ** NOVAS FUN√á√ïES DE INFER√äNCIA COPIADAS DO SEXO.PY **
# -----------------------------------------------------------------------
def extrair_primeiro_nome(nome_completo):
    """Extrai e limpa o primeiro nome da string completa."""
    if pd.isna(nome_completo):
        return None
    nome = str(nome_completo).strip()
    # Limpeza agressiva: remove tudo que n√£o for letra ou espa√ßo
    nome_limpo_completo = re.sub(r'[^a-zA-Z\s]', ' ', nome) 
    palavras = [p for p in nome_limpo_completo.split() if p]
    if not palavras:
        return None
    return palavras[0].capitalize()

def inferir_sexo_br(primeiro_nome):
    """Inferir o sexo usando a biblioteca gender-guesser-br (IBGE)."""
    if not primeiro_nome:
        return None 
    
    try:
        # A API do IBGE pode falhar ou demorar. O try/except √© crucial aqui.
        resultado = Genero(primeiro_nome)() 
        
        # Traduz os resultados: 'masculino', 'feminino' ou 'N√£o Encontrado'
        if resultado == 'masculino':
            return 'Masculino'
        elif resultado == 'feminino':
            return 'Feminino'
        else: # 'N√£o Encontrado'
            return None # Retorna None para indicar que n√£o houve sucesso
            
    except Exception as e:
        # print(f"Erro ao consultar nome {primeiro_nome}: {e}") # Descomente para debug
        return None 
# -----------------------------------------------------------------------


def carregar_dados_sexo(caminho_arquivo):
    """Carrega o arquivo de sexo (Excel) e prepara o DataFrame para merge."""
    print(f"\n-> Carregando dados de Sexo de: {os.path.basename(caminho_arquivo)}")
    try:
        # Tenta carregar o arquivo Excel
        df_sexo = pd.read_excel(caminho_arquivo)
        
        # Garante que as colunas 'Nome' e 'Sexo' existam e limpa nomes de colunas
        df_sexo.columns = df_sexo.columns.str.strip()

        if 'Nome' not in df_sexo.columns or 'Sexo' not in df_sexo.columns:
            print("  ERRO: O arquivo de sexo n√£o cont√©m as colunas 'Nome' e/ou 'Sexo'.")
            return None

        # Normaliza valores de Sexo (e.g., Desconhecido/Ambos para NaN, ou usa somente Masculino/Feminino)
        # O melhor √© deixar o valor como est√° para ter a refer√™ncia manual
        df_sexo = df_sexo[['Nome', 'Sexo']].copy()
        
        # Limpa espa√ßos em branco nos Nomes do arquivo de Sexo para garantir o merge
        df_sexo['Nome'] = df_sexo['Nome'].astype(str).str.strip().str.upper() # Adiciona .upper()
        df_sexo.drop_duplicates(subset=['Nome'], keep='first', inplace=True)
        
        # Tratamento: Converte 'Desconhecido', 'Ambos' ou string vazia para NA para ser preenchido pela infer√™ncia
        valores_a_anular = ['DESCONHECIDO', 'AMBOS', '']
        df_sexo['Sexo'] = df_sexo['Sexo'].astype(str).str.upper()
        df_sexo.loc[df_sexo['Sexo'].isin(valores_a_anular), 'Sexo'] = pd.NA
        
        # Converte o resto de volta para Capitalize/Normal
        df_sexo['Sexo'] = df_sexo['Sexo'].str.capitalize()

        print(f"  Sucesso: {len(df_sexo)} nomes √∫nicos carregados com dados de Sexo.")
        return df_sexo
        
    except FileNotFoundError:
        print(f"  AVISO: Arquivo de sexo n√£o encontrado no caminho: {caminho_arquivo}")
        return None
    except Exception as e:
        print(f"  ERRO ao processar o arquivo de sexo (EXCEL): {e}")
        return None


def transformar_e_selecionar(caminho_arquivo_local, colunas_desejadas):
    """Carrega o arquivo (CSV ou XLS/XLSX), trata, renomeia e seleciona colunas."""
    nome_arquivo = os.path.basename(caminho_arquivo_local)
    
    # Verifica a extens√£o para decidir como ler (prioriza CSV/separadores, sen√£o tenta Excel)
    if nome_arquivo.lower().endswith('.xls') or nome_arquivo.lower().endswith('.xlsx'):
        print(f"  -> Processando com Pandas: {nome_arquivo} (Lendo como Excel)")
        try:
            df = pd.read_excel(caminho_arquivo_local)
        except Exception as e:
            print(f"  ERRO CR√çTICO ao ler {nome_arquivo} como Excel: {e}")
            return None
    else:
        print(f"  -> Processando com Pandas: {nome_arquivo} (Lendo como CSV, tentando delimitadores)")
        # 1. Tenta carregar o arquivo CSV, testando delimitadores
        try:
            df = pd.read_csv(caminho_arquivo_local, sep=';', encoding='latin1') 
        except Exception:
            try:
                df = pd.read_csv(caminho_arquivo_local, sep=',', encoding='latin1')
            except Exception as e:
                print(f"  ERRO CR√çTICO ao ler {nome_arquivo} como CSV. Falha com ';' e ','.")
                print(f"  Detalhes: {e}")
                return None

    # NOVO PASSO: LIMPEZA E PADRONIZA√á√ÉO DOS NOMES DAS COLUNAS DO ARQUIVO
    df.columns = df.columns.str.strip()
    colunas_reais = df.columns.tolist()
    
    # --- BLOCO DE MUDAN√áA DE NOME E TRATAMENTO DE AUSENTES ---
    
    # 2. Mapeamento de Colunas Ausentes (Sal√°rio)
    # Tenta renomear 'Sal√°rio Simulado' para 'Valor Sal√°rio'
    if 'Valor Sal√°rio' not in colunas_reais and 'Sal√°rio Simulado' in colunas_reais:
        df.rename(columns={'Sal√°rio Simulado': 'Valor Sal√°rio'}, inplace=True)
        print(f"  AVISO: 'Sal√°rio Simulado' renomeado para 'Valor Sal√°rio' em {nome_arquivo}.")
    
    # 3. Lista de Colunas Dispon√≠veis
    # Filtra COLUNAS_DESEJADAS (agora mantendo 'Sexo', pois ser√° a coluna de destino)
    
    colunas_presentes = [col.strip() for col in colunas_desejadas if col.strip() in df.columns]

    colunas_ausentes = [col.strip() for col in colunas_desejadas if col.strip() not in df.columns]
    
    if colunas_ausentes:
        print(f"  AVISO: Colunas n√£o encontradas e IGNORADAS em {nome_arquivo}: {colunas_ausentes}")


    # 4. Sele√ß√£o de Colunas
    try:
        df_selecionado = df[colunas_presentes].copy()
        
        # Limpa espa√ßos em branco nos Nomes do DataFrame principal e coloca em caixa alta
        if 'Nome' in df_selecionado.columns:
            df_selecionado['Nome'] = df_selecionado['Nome'].astype(str).str.strip().str.upper()
        return df_selecionado
    except KeyError as e:
        print(f"  ERRO: Sele√ß√£o final falhou ap√≥s o mapeamento. Detalhes: {e}")
        return None

def limpar_chave(df, coluna_id):
    """Garante que IDs/Chaves sejam strings, sem espa√ßos e em caixa alta."""
    if coluna_id in df.columns:
        # 1. Tenta remover o '.0' de n√∫meros lidos como float/int antes de converter para string
        if df[coluna_id].dtype in ['int64', 'float64']:
            # Use uma express√£o regular mais segura para remover apenas .0 no final
            df[coluna_id] = df[coluna_id].astype(str).str.replace(r'\.0$', '', regex=True)
        # 2. Converte para string, remove espa√ßos e coloca em caixa alta
        df[coluna_id] = df[coluna_id].astype(str).str.strip().str.upper().str.replace('NAN', '') # Remove 'NAN' residual
    return df

def calcular_idade_faixa_etaria(df):
    """Calcula Idade Atual e Faixa Et√°ria no DataFrame Pessoa."""
    if 'Nascimento' in df.columns:
        print("Calculando 'Idade Atual' e 'Faixa Et√°ria'...")
        hoje = pd.to_datetime(date.today())
        
        # Garante que 'Nascimento' esteja como datetime antes de calcular
        df['Nascimento'] = pd.to_datetime(df['Nascimento'], errors='coerce', dayfirst=True)
        
        df['Idade Atual'] = (hoje - df['Nascimento']).dt.days / 365.25
        df['Idade Atual'] = df['Idade Atual'].apply(lambda x: int(x) if pd.notna(x) else pd.NA).astype('Int64')

        bins = [0, 20, 25, 30, 35, 40, 50, 60, 100]
        labels = [
            "1. Abaixo de 20", "2. 20 - 24 Anos", "3. 25 - 29 Anos", "4. 30 - 34 Anos",
            "5. 35 - 39 Anos", "6. 40 - 49 Anos", "7. 50 - 59 Anos", "8. 60 ou Mais"
        ]
        
        # Usa .fillna() para Faixa Et√°ria inv√°lida
        df['Faixa Et√°ria (Pir√¢mide)'] = pd.cut(
            df['Idade Atual'],
            bins=bins,
            labels=labels,
            right=False
        ).astype(str).str.replace('nan', '9. Idade Inv√°lida')
        print("Colunas 'Idade Atual' e 'Faixa Et√°ria' adicionadas √† Dim_Pessoa.")
    else:
        print("AVISO: Coluna 'Nascimento' n√£o encontrada. Idade n√£o calculada.")
    return df

# ***********************************************************************
# NOVA FUN√á√ÉO PARA PROCESSAR CSVs AUXILIARES (FALTAS/ABSENTE√çSMO)
# ***********************************************************************
def etl_processa_csv_auxiliar(caminho_arquivo, nome_tabela):
    """
    Fun√ß√£o dedicada para ler e processar arquivos CSV auxiliares (como Faltas ou Absente√≠smo).
    Faz a limpeza b√°sica, padroniza√ß√£o do Nome, limpeza de Hor√°rios, convers√£o de ABS e de Data.
    """
    print(f"\n-> Processando arquivo auxiliar: {os.path.basename(caminho_arquivo)} para a tabela {nome_tabela}")
    try:
        # Tenta ler o CSV, usando latin1 por ser comum em dados brasileiros
        # Mantendo o sep=';' da sua √∫ltima tentativa.
        df = pd.read_csv(caminho_arquivo, sep=';', encoding='latin1')
    except FileNotFoundError:
        print(f"  AVISO: Arquivo '{os.path.basename(caminho_arquivo)}' n√£o encontrado. Pulando.")
        return None
    except Exception as e:
        try:
            # Tenta com v√≠rgula como separador (fallback)
            df = pd.read_csv(caminho_arquivo, sep=',', encoding='latin1')
        except Exception as e:
            print(f"  ERRO CR√çTICO ao ler {os.path.basename(caminho_arquivo)} como CSV. Detalhes: {e}")
            return None

    # Limpeza b√°sica e padroniza√ß√£o dos cabe√ßalhos das colunas
    df.columns = df.columns.str.strip().str.replace(r'[^a-zA-Z0-9\s\(\)%]', '', regex=True)
    
    # Padroniza√ß√£o da coluna Nome
    if 'Nome' in df.columns:
        df['Nome'] = df['Nome'].astype(str).str.strip().str.upper()
        print("  Coluna 'Nome' padronizada (Upper, Strip).")
        
    # --------------------------------------------------------------------------------
    # üåü 1. NOVO: LIMPEZA DAS COLUNAS DE HOR√ÅRIO (Removendo o :00 final)
    # --------------------------------------------------------------------------------
    colunas_horario = ['Previsto', 'Ausencia', 'Presenca']
    
    for col in colunas_horario:
        # Verifica se a coluna existe
        if col in df.columns:
            # Remove o padr√£o ":00" APENAS se estiver no final da string (usando regex '$')
            df[col] = df[col].astype(str).str.replace(r':00$', '', regex=True)
            # Remove ":00:00" em colunas de Absente√≠smo.
            df[col] = df[col].astype(str).str.replace(r'00:00$', '00', regex=True)
            print(f"  Coluna '{col}' limpa (removido o sufixo :00).")
    # --------------------------------------------------------------------------------
    
    # --------------------------------------------------------------------------------
    # üåü 2. NOVO: CONVERS√ÉO DA COLUNA 'ABS' PARA N√öMERO DECIMAL (%)
    # --------------------------------------------------------------------------------
    if 'ABS' in df.columns:
        try:
            # 1. Limpa o '%' e espa√ßos
            df['ABS'] = df['ABS'].astype(str).str.replace('%', '', regex=False).str.strip()
            
            # 2. Substitui a v√≠rgula (,) por ponto (.) como separador decimal
            df['ABS'] = df['ABS'].str.replace(',', '.', regex=False)
            
            # 3. Converte para n√∫mero e divide por 100 (para decimal: 5.88% -> 0.0588)
            df['ABS'] = pd.to_numeric(df['ABS'], errors='coerce') / 100
            print("  Coluna 'ABS' convertida para n√∫mero decimal (%).")
        except Exception as e:
            print(f"  AVISO: Falha ao converter coluna 'ABS' para num√©rico. Detalhes: {e}")
    # --------------------------------------------------------------------------------
    
    # Convers√£o da coluna Data (mantida a corre√ß√£o anterior)
    if 'Data' in df.columns:
        
        # 1. REMOVER O DIA DA SEMANA
        # O padr√£o '.*,' busca qualquer coisa (.*) at√© a primeira v√≠rgula (,) e a pr√≥pria v√≠rgula.
        df['Data'] = df['Data'].astype(str).str.replace(r'.*,', '', regex=True).str.strip()
        
        # 2. CONVERS√ÉO FINAL PARA DATETIME
        df['Data'] = pd.to_datetime(df['Data'], errors='coerce', dayfirst=True)
        print("  Coluna 'Data' limpa (dia da semana removido) e convertida para DateTime.")
        
    print(f"  Tabela '{nome_tabela}' processada com {len(df)} linhas.")
    return df


# =======================================================================
# 3. L√ìGICA PRINCIPAL: CONSOLIDA√á√ÉO (ETL)
# =======================================================================

def etl_consolida_e_salva_csv(colunas_desejadas):
    """Busca arquivos, processa, consolida, faz o merge de Sexo e salva o CSV intermedi√°rio."""
    
    lista_dataframes = []
    
    print("\n" + "="*70)
    print(f"ETAPA 1/2: CONSOLIDA√á√ÉO E INFER√äNCIA DE SEXO | In√≠cio: {time.ctime()}")
    print("="*70)

    # PASSO 0: Carregar Dados Auxiliares (Sexo)
    df_sexo = carregar_dados_sexo(ARQUIVO_SEXO) 
    
    # 1. BUSCAR TODOS OS ARQUIVOS DA PASTA
    try:
        todos_arquivos = [
            os.path.join(PASTA_ORIGEM_ONEDRIVE, f)
            for f in os.listdir(PASTA_ORIGEM_ONEDRIVE)
            if os.path.isfile(os.path.join(PASTA_ORIGEM_ONEDRIVE, f)) and not f.startswith('.') 
        ]
    except FileNotFoundError:
        print(f"\nERRO: Pasta de origem n√£o encontrada: {PASTA_ORIGEM_ONEDRIVE}. Execute 'ambiente_de_teste.py' primeiro.")
        return None

    # 2. FILTRA OS ARQUIVOS QUE PROVAVELMENTE PODEM SER CSVs/XLSs
    arquivos_compativeis = [
        f for f in todos_arquivos 
        if f.lower().endswith(('.xls', '.xlsx', '.csv'))
    ]

    if not arquivos_compativeis:
        print("\n" + "="*70)
        print("NENHUM ARQUIVO COMPAT√çVEL (.xls, .xlsx ou .csv) ENCONTRADO na pasta especificada.")
        print(f"Caminho verificado: {PASTA_ORIGEM_ONEDRIVE}")
        print("="*70)
        return None

    print(f"-> {len(arquivos_compativeis)} arquivos compat√≠veis encontrados. Iniciando consolida√ß√£o...")
    
    # 3. Processamento e Consolida√ß√£o
    for arquivo in arquivos_compativeis:
        # A coluna 'Sexo' pode vir aqui, mas ser√° substitu√≠da pelo merge/infer√™ncia
        df_processado = transformar_e_selecionar(arquivo, colunas_desejadas) 
        
        if df_processado is not None and not df_processado.empty:
            df_processado['Origem_Arquivo'] = os.path.basename(arquivo)
            lista_dataframes.append(df_processado)

    # 4. Combina√ß√£o Final (Concatenar)
    if lista_dataframes:
        CONTEUDO_FINAL = pd.concat(lista_dataframes, ignore_index=True, sort=False)
        
        # --- PASSO 5: MERGE E INFER√äNCIA CONDICIONAL DE SEXO (L√ìGICA UNIFICADA) ---
        
        # 5.1 PREPARA√á√ÉO: Remove qualquer coluna 'Sexo' que tenha vindo dos arquivos brutos
        CONTEUDO_FINAL.drop(columns=['Sexo'], inplace=True, errors='ignore')
        
        # Adiciona uma coluna 'Sexo' vazia para ser preenchida
        if 'Sexo' not in CONTEUDO_FINAL.columns:
            CONTEUDO_FINAL['Sexo'] = pd.NA

        # 5.2 MERGE COM DADOS DE SEXO (Excel) - Prioridade Manual
        if df_sexo is not None and 'Nome' in CONTEUDO_FINAL.columns:
            print("\n-> Realizando Merge com a informa√ß√£o de Sexo do Excel (Prioridade Manual)...")
            
            # Merge Left Join
            df_merged = pd.merge(
                CONTEUDO_FINAL.drop(columns=['Sexo'], errors='ignore'), 
                df_sexo,
                on='Nome',
                how='left'
            )
            # Define o resultado do merge como o CONTEUDO_FINAL tempor√°rio
            CONTEUDO_FINAL = df_merged
            
            print(f"  Merge conclu√≠do. {CONTEUDO_FINAL['Sexo'].count()} valores de Sexo manual/existente carregados.")
        else:
            print("  AVISO: Arquivo de Sexo manual n√£o carregado. Pulando merge.")

        # 5.3 INFER√äNCIA CONDICIONAL: Aplica gender-guesser-br APENAS nos valores nulos.
        
        # 5.3.1 Identifica os registros SEM Sexo (Nulos)
        # O .isna() trata valores NA do Pandas e None
        condicao_inferir = CONTEUDO_FINAL['Sexo'].isna()
        df_a_inferir = CONTEUDO_FINAL[condicao_inferir].copy()
        
        if not df_a_inferir.empty:
            total_a_inferir = len(df_a_inferir)
            print(f"\n  -> Aplicando Infer√™ncia do IBGE (Lenta) a {total_a_inferir} registros NA/Nulos...")

            # 5.3.2 Aplica a fun√ß√£o de limpeza de nome
            df_a_inferir['Primeiro_Nome'] = df_a_inferir['Nome'].apply(extrair_primeiro_nome)
            
            # 5.3.3 Aplica a fun√ß√£o de infer√™ncia
            novos_sexos = df_a_inferir['Primeiro_Nome'].apply(inferir_sexo_br)
            
            # 5.3.4 Atualiza o DataFrame principal (CONTEUDO_FINAL)
            sucesso_br = novos_sexos.dropna()
            
            # O .loc garante que os novos valores sejam colocados DE VOLTA no DataFrame principal, APENAS nos nulos
            CONTEUDO_FINAL.loc[sucesso_br.index, 'Sexo'] = sucesso_br
            
            print(f"  {sucesso_br.count()} valores de Sexo preenchidos por Infer√™ncia.")
        else:
            print("  Nenhum valor nulo (NA) de Sexo restante para infer√™ncia. Passo ignorado.")
        
        # --- FIM DO PASSO 5 ---

        # 6. Adiciona as colunas ausentes na consolida√ß√£o final (se alguma faltou em todos)
        # Garante que todas as colunas desejadas estejam presentes
        for col in [str(coluna).strip() for coluna in colunas_desejadas if coluna is not None]:
            if col not in CONTEUDO_FINAL.columns:
                CONTEUDO_FINAL[col] = pd.NA
        
        # 7. Salvar o Resultado Intermedi√°rio (CSV)
        colunas_finais_ordenadas = [c.strip() for c in colunas_desejadas] + ['Origem_Arquivo']
        colunas_para_salvar = [col for col in colunas_finais_ordenadas if col in CONTEUDO_FINAL.columns]

        CONTEUDO_FINAL[colunas_para_salvar].to_csv(ARQUIVO_INTERMEDIARIO_CSV, index=False, encoding='utf-8')
        
        print("\n" + "="*70)
        print(f"ETAPA 1/2 COMPLETA! CSV Intermedi√°rio salvo em: {ARQUIVO_INTERMEDIARIO_CSV}")
        print(f"Total de linhas consolidadas (incluindo duplicadas): {len(CONTEUDO_FINAL)}")
        print("="*70)
        return CONTEUDO_FINAL

    else:
        print("\nNenhum arquivo p√¥de ser processado com sucesso.")
        return None

# =======================================================================
# 4. L√ìGICA PRINCIPAL: MODELAGEM (ELT)
# =======================================================================

def etl_modela_e_salva_excel(df_input, colunas_desejadas):
    """
    Recebe o DataFrame consolidado, limpa, cria o modelo estrela/snowflake 
    e salva no arquivo final Excel de m√∫ltiplas abas, incluindo tabelas auxiliares.
    """
    if df_input is None or df_input.empty:
        print("ERRO: O DataFrame consolidado est√° vazio ou n√£o foi gerado.")
        return
        
    print("\n" + "="*70)
    print("ETAPA 2/2: MODELAGEM E CRIA√á√ÉO DE MODELO ESTRELA/SNOWFLAKE")
    print("="*70)

    df = df_input.copy()
    
    # ********************************************************************
    # ** NOVO PASSO: PROCESSAMENTO DAS TABELAS AUXILIARES **
    # ********************************************************************
    df_faltas = etl_processa_csv_auxiliar(ARQUIVO_FALTAS, 'Fato_Faltas')
    df_abs = etl_processa_csv_auxiliar(ARQUIVO_ABS, 'Fato_Absenteismo')


    # 1. SELE√á√ÉO DE COLUNAS
    colunas_para_selecao = [col for col in colunas_desejadas if col in df.columns]
    df = df[colunas_para_selecao + ['Origem_Arquivo']].copy()
    print(f"DataFrame filtrado para {len(df.columns)} colunas desejadas e existentes.")

    # 2. LIMPEZA E TRATAMENTO DE DADOS

    # A. Aplica Limpeza √†s Colunas que Ser√£o Chaves (IDs)
    print("\nIniciando limpeza e padroniza√ß√£o das chaves de relacionamento...")
    # ** ALTERA√á√ÉO: Adiciona 'CPF' √† lista de chaves a serem limpas
    colunas_para_limpar = ['Nome', 'CPF', 'Empresa', 'Cadastro', 'Cargo', 'C.Custo', 'Filial']
    for col in colunas_para_limpar:
        df = limpar_chave(df, col)
    print("Limpeza de chaves de relacionamento (strip e upper) conclu√≠da.")

    # B. Limpeza de Linhas VAZIAS (Remove linhas onde a chave √∫nica de pessoa ('CPF') est√° nula ou vazia)
    df.replace('', pd.NA, inplace=True) # Substitui strings vazias por NA para o dropna
    
    # ** ALTERA√á√ÉO: Garante que CPF (nova chave √∫nica) e Nome (descri√ß√£o essencial) existam
    df.dropna(subset=['CPF', 'Nome'], inplace=True)
    
    # Remove valores string 'NAN' nas chaves
    df = df[df['CPF'] != 'NAN'] 
    df = df[df['Nome'] != 'NAN'] 
    print(f"Linhas com CPF e/ou Nome vazios removidas. Linhas restantes: {len(df)}")


    # C. Preenchimento de nulos em colunas espec√≠ficas
    if 'Descri√ß√£o (T. Adm)' in df.columns:
        df['Descri√ß√£o (T. Adm)'] = df['Descri√ß√£o (T. Adm)'].fillna('N√ÉO INFORMADO')
        print("Valores nulos em 'Descri√ß√£o (T. Adm)' preenchidos.")
        
    # NOVO: Preenche Sexo nulo com valor 'INFER√äNCIA FALHOU' (para fins de visualiza√ß√£o)
    if 'Sexo' in df.columns:
        df['Sexo'] = df['Sexo'].fillna('N√£o Definido/Inferido')
        print("Valores nulos em 'Sexo' preenchidos com 'N√£o Definido/Inferido'.")


    # 3. CONVERS√ÉO DE DATAS
    COLUNAS_DE_DATA = [col for col in colunas_desejadas if 'Data' in col or 'Admiss√£o' in col or 'Nascimento' in col or '√öltima Simula√ß√£o' in col]
    print("\nIniciando convers√£o de colunas de data...")
    for col in COLUNAS_DE_DATA:
        if col in df.columns:
            # For√ßa o tratamento como data no formato brasileiro dia/m√™s/ano
            df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)
    print("Convers√£o de datas conclu√≠da.")

    # ===================================================================
    # 4. GERA√á√ÉO DAS TABELAS DIMENS√ÉO
    # ===================================================================
    print("\nGerando Tabelas Dimens√£o com ID Sequencial (Chaves Suplementares)...")

    # --- DIMENS√ÉO 5: Dim_Empresa (PAI) ---
    dim_empresa = df[['Empresa', 'Nome (Empresa)']].copy().dropna(subset=['Empresa'])
    dim_empresa.drop_duplicates(subset=['Empresa'], inplace=True)
    dim_empresa.insert(0, 'Empresa_ID', range(1, len(dim_empresa) + 1))
    print(f"Dimens√£o Empresa (Pai) criada com {len(dim_empresa)} registros √∫nicos.")

    # --- DIMENS√ÉO 4: Dim_Filial (FILHO) ---
    colunas_filial_origem = ['Filial', 'Apelido (Filial)', 'Empresa']
    dim_filial = df[[col for col in colunas_filial_origem if col in df.columns]].copy().dropna(subset=['Filial'])
    dim_filial.drop_duplicates(subset=['Filial', 'Empresa'], inplace=True)

    if 'Apelido (Filial)' in dim_filial.columns:
        dim_filial['Apelido (Filial)'] = dim_filial['Apelido (Filial)'].fillna('N√ÉO INFORMADO')

    dim_filial.insert(0, 'Filial_ID', range(1, len(dim_filial) + 1))

    # JUNTA COM Dim_Empresa para obter a Chave Estrangeira (FK)
    dim_filial = pd.merge(dim_filial, dim_empresa[['Empresa', 'Empresa_ID']], on='Empresa', how='left')
    dim_filial.drop(columns=['Empresa'], inplace=True, errors='ignore')
    print(f"Dimens√£o Filial (Filho) criada com {len(dim_filial)} registros √∫nicos e ligada √† Empresa (FK Empresa_ID).")


    # --- DIMENS√ÉO 1: Dim_Pessoa ---
    colunas_pessoa = [
        'Nome', 'CPF', 'Cadastro', 'Nascimento', 'Sexo', # ** CPF adicionado aqui **
        'Estado Civil', 'Descri√ß√£o (Estado Civil)',
        'Instru√ß√£o', 'Descri√ß√£o (Instru√ß√£o)', 'Ra√ßa/Etnia', 'Descri√ß√£o (Ra√ßa/Etnia)',
        'Dependentes IR', 'Dependentes Saf', 'Dep. Saldo FGTS', 'Cadastramento PIS',
        'Nome (Cadastro O. Contrato)'
    ]
    # Seleciona apenas as colunas que existem no df
    cols_existentes_pessoa = [col for col in colunas_pessoa if col in df.columns]
    
    dim_pessoa = df[cols_existentes_pessoa].copy()
    
    # ** ALTERA√á√ÉO: Usa CPF como chave √∫nica para drop_duplicates **
    if 'CPF' in dim_pessoa.columns:
        dim_pessoa.drop_duplicates(subset=['CPF'], inplace=True)
        print("Chave √∫nica para Dim_Pessoa definida como CPF.")
    else:
        # Fallback
        dim_pessoa.drop_duplicates(subset=['Nome', 'Cadastro'], inplace=True)
        print("ALERTA: Coluna 'CPF' n√£o encontrada. Usando ['Nome', 'Cadastro'] como chave de Pessoa.")
    
    dim_pessoa.insert(0, 'Pessoa_ID', range(1, len(dim_pessoa) + 1))
    
    # ********************************************************************
    # ** C√ÅLCULO DE IDADE E FAIXA ET√ÅRIA **
    # ********************************************************************
    dim_pessoa = calcular_idade_faixa_etaria(dim_pessoa)
    
    print(f"Dimens√£o Pessoa criada com {len(dim_pessoa)} registros √öNICOS, incluindo 'Sexo'.")

    # --- DIMENS√ÉO 2: Dim_Cargo ---
    dim_cargo = df[['Cargo', 'T√≠tulo Reduzido (Cargo)']].copy().dropna(subset=['Cargo'])
    dim_cargo.drop_duplicates(subset=['Cargo'], inplace=True)
    dim_cargo.insert(0, 'Cargo_ID', range(1, len(dim_cargo) + 1))
    print(f"Dimens√£o Cargo criada com {len(dim_cargo)} registros √∫nicos.")

    # --- DIMENS√ÉO 3: Dim_CCusto ---
    dim_ccusto = df[['C.Custo', 'Descri√ß√£o (C.Custo)']].copy().dropna(subset=['C.Custo'])
    dim_ccusto.drop_duplicates(subset=['C.Custo'], inplace=True)
    dim_ccusto.insert(0, 'CCusto_ID', range(1, len(dim_ccusto) + 1))
    print(f"Dimens√£o C.Custo criada com {len(dim_ccusto)} registros √∫nicos.")


    # ===================================================================
    # 5. CRIA√á√ÉO DA TABELA FATO (Fato_Contratos)
    # ===================================================================
    print("\nPreparando a Tabela Fato com todos os novos IDs...")
    df_fato = df.copy()

    # 1. Merges Padr√£o (Pessoa, Cargo, CCusto, Empresa, Filial)
    
    # ** ALTERA√á√ÉO: Merge Pessoa (Chave √önica: CPF) **
    chave_merge_pessoa = ['CPF']
    cols_merge_pessoa = chave_merge_pessoa + ['Pessoa_ID']

    df_fato = pd.merge(df_fato, dim_pessoa[cols_merge_pessoa], 
                             on=chave_merge_pessoa, how='left', suffixes=('', '_Pessoa_ID_drop'))
    df_fato.drop(columns=[c for c in df_fato.columns if '_Pessoa_ID_drop' in c], inplace=True, errors='ignore')
    
    # Merge Cargo
    df_fato = pd.merge(df_fato, dim_cargo[['Cargo', 'Cargo_ID']], on='Cargo', how='left')
    
    # Merge C.Custo
    df_fato = pd.merge(df_fato, dim_ccusto[['C.Custo', 'CCusto_ID']], on='C.Custo', how='left')

    # Merge Empresa (Obt√©m Empresa_ID)
    df_fato = pd.merge(df_fato, dim_empresa[['Empresa', 'Empresa_ID']], on='Empresa', how='left')

    # Merge Filial (Obt√©m Filial_ID) - Chave Composta: Filial + Empresa_ID (Snowflake)
    dim_filial_chaves = dim_filial[['Filial_ID', 'Filial', 'Empresa_ID']]

    df_fato = pd.merge(
        df_fato, 
        dim_filial_chaves, 
        on=['Filial', 'Empresa_ID'], 
        how='left',
        suffixes=('_drop', '') 
    )

    print("Todos os novos IDs (Surrogate Keys) foram adicionados √† Tabela Fato.")

    # 2. Sele√ß√£o e Limpeza da Tabela Fato Final
    # Remo√ß√£o das colunas originais que viraram IDs/Chaves naturais
    # ** ALTERA√á√ÉO: Adiciona 'CPF' √† lista de colunas a serem descartadas **
    colunas_para_descartar = ['Nome', 'CPF', 'Cargo', 'C.Custo', 'Empresa', 'Filial', 'Sexo']
    df_fato.drop(columns=[col for col in colunas_para_descartar if col in df_fato.columns], 
                 inplace=True, errors='ignore')


    # B. Selecionar colunas para a Tabela Fato
    colunas_fato_desejadas = [
        # NOVOS IDs (Chaves Estrangeiras)
        'Pessoa_ID', 'Cargo_ID', 'CCusto_ID', 'Filial_ID', 'Empresa_ID', 
        # CHAVE DE AUDITORIA
        'Cadastro', 
        # Dados de Contrato/Fato
        'Admiss√£o', 'Data Afastamento', 'Data Sal√°rio', 'Data Cargo', 'Data C.Custo',
        'Data de Reintegra√ß√£o', 'Data V√≠nculo', '√öltima Simula√ß√£o', 'Data Inclus√£o', 
        '% Desempenho', '% Insalubridade', '% Periculosidade', '% Reajuste', 
        '% FGTS', '% ISS', 'Dependentes IR', 'Dependentes Saf',
        'Situa√ß√£o', 'Descri√ß√£o (Situa√ß√£o)', 'Causa', 'Descri√ß√£o (Causa)', 'Escala',
        'Descri√ß√£o (Escala)', 'Op√ß√£o FGTS', 'Per√≠odo Pagto', 'Descri√ß√£o (Per√≠odo Pagto)',
        'Descri√ß√£o (T. Adm)', 'Descri√ß√£o (T. Contrato)', 'Descri√ß√£o (Cat. eSocial)',
        'Descri√ß√£o (Motivo Alt. Sal√°rio)', 'Recebe 13¬∞ Sal√°rio', 'C√≥digo Fornecedor',
        'Origem_Arquivo' # Mant√©m a coluna de auditoria do arquivo original
    ]

    df_fato_final = df_fato[[col for col in colunas_fato_desejadas if col in df_fato.columns]].copy()
    print("Tabela Fato final criada com as colunas de IDs e medidas.")

    # ===================================================================
    # 6. SALVAMENTO EM M√öLTIPLAS ABAS (EXCEL)
    # ===================================================================
    
    # --- ADI√á√ÉO DE M√âTRICAS FINAIS SOLICITADAS ---
    total_colaboradores_unicos = len(dim_pessoa)
    total_contratos = len(df_fato_final)
    
    print("\n" + "#"*70)
    print("M√âTRICAS CHAVE DO MODELO DE DADOS")
    print(f"1. Total de Contratos/Registros (Tabela Fato): {total_contratos}")
    print(f"2. Total de Colaboradores √önicos (Baseado em CPF): {total_colaboradores_unicos}")
    print("#"*70)
    
    print(f"\nSalvando Modelo Estrela/Snowflake em EXCEL: {ARQUIVO_FINAL_MODELADO} (M√∫ltiplas abas)")
    try:
        with pd.ExcelWriter(ARQUIVO_FINAL_MODELADO, engine='xlsxwriter') as writer:
            df_fato_final.to_excel(writer, sheet_name='Fato_Contratos', index=False)
            
            # Dimens√µes
            # ** ALTERA√á√ÉO: Dim_Pessoa agora mant√©m Nome e Cadastro como atributos, CPF √© a chave natural **
            dim_pessoa.to_excel(writer, sheet_name='Dim_Pessoa', index=False)

            dim_cargo.drop(columns=['Cargo']).to_excel(writer, sheet_name='Dim_Cargo', index=False)
            dim_ccusto.drop(columns=['C.Custo']).to_excel(writer, sheet_name='Dim_CCusto', index=False)

            # Dimens√µes Empresa e Filial (Hier√°rquicas)
            dim_filial.to_excel(writer, sheet_name='Dim_Filial', index=False) 
            dim_empresa.drop(columns=['Empresa']).to_excel(writer, sheet_name='Dim_Empresa', index=False) 
            
            # ********************************************************************
            # ** SALVANDO AS NOVAS TABELAS AUXILIARES **
            # ********************************************************************
            if df_faltas is not None:
                df_faltas.to_excel(writer, sheet_name='Fato_Faltas', index=False)
                print("Tabela auxiliar 'Fato_Faltas' salva.")
            if df_abs is not None:
                df_abs.to_excel(writer, sheet_name='Fato_Absenteismo', index=False)
                print("Tabela auxiliar 'Fato_Absenteismo' salva.")
            
            # ********************************************************************

        print("\n" + "="*70)
        print("Modelagem e salvamento CONCLU√çDOS com sucesso!")
        print(f"O arquivo EXCEL '{ARQUIVO_FINAL_MODELADO}' (6 + Auxiliares abas) est√° pronto.")
        print("Instru√ß√£o para Power BI: As chaves 'Empresa_ID' e 'Filial_ID' ligam as dimens√µes √† Fato, formando o Snowflake/Estrela.")
        print("="*70)

    except Exception as e:
        print(f"\nERRO ao salvar o arquivo Excel: {e}")


# =======================================================================
# 5. EXECU√á√ÉO DO FLUXO COMPLETO
# =======================================================================
def run_full_etl():
    """Executa a consolida√ß√£o seguida da modelagem."""
    
    # 1. Executa a Etapa de Consolida√ß√£o e salva o CSV intermedi√°rio
    df_consolidado = etl_consolida_e_salva_csv(COLUNAS_DESEJADAS)
    
    # 2. Executa a Etapa de Modelagem
    if df_consolidado is not None:
        etl_modela_e_salva_excel(df_consolidado, COLUNAS_DESEJADAS)

if __name__ == "__main__":
    # Verifica√ß√£o simples para lembrar de rodar o setup, caso a pasta de origem n√£o exista
    if not os.path.exists(PASTA_ORIGEM_ONEDRIVE) or not os.path.exists(ARQUIVO_SEXO):
        print("ALERTA: O ambiente de teste n√£o foi encontrado. Verifique PASTA_ORIGEM_ONEDRIVE e ARQUIVO_SEXO.")
    else:
        run_full_etl()